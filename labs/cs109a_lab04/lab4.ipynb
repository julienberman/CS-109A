{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS1090A Introduction to Data Science \n",
                "\n",
                "## Lab 4: Feature Engineering and Pipelines\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2024**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
                "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, FunctionTransformer \n",
                "from sklearn.metrics import r2_score, mean_squared_error \n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.neighbors import KNeighborsRegressor\n",
                "from pandas.api.types import CategoricalDtype \n",
                "from sklearn.compose import make_column_transformer, TransformedTargetRegressor\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline, make_pipeline\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "import seaborn as sns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âš½ðŸ’° Can we predict the \"market value\" of prospective players in the Fantasy Premier League?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Table of Contents:\n",
                "- Inspecting the dataset\n",
                "    - Selecting relevant predictors\n",
                "- Pre-processing\n",
                "    - One-hot Encoding of Categorical Variables\n",
                "    - Scaling of Quantitative Variables (Standardization)\n",
                "- Feature Engineering\n",
                "    - Polynomial and Interaction Terms\n",
                "- Train-Val-Test Split\n",
                "    - Stratified splitting\n",
                "- Modeling\n",
                "    - Multi-Linear Regression\n",
                "    - KNN"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**The Story**\n",
                "\n",
                "Once Upon a Time, the Football Association gave us a dataset and asked us to help them predict the \u003cb\u003emarket value\u003c/b\u003e (what the player could earn when hired by a new team) of prospective players.\n",
                "\n",
                "**The dataset**\n",
                "\n",
                "The dataset includes data up to 2017, and was created by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) who used a variety of sources, including *transfermrkt.com* and *Fantasy Premier League (FPL)*, and a variety of methods, including scraping. \n",
                "Each observation is a collection of facts about players in the English Premier League. \n",
                "\n",
                "`name`             : Name of the player  \n",
                "`club`             : Club of the player  \n",
                "`age`              : Age of the player  \n",
                "`position`         : The usual position on the pitch  \n",
                "`position_cat`     : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers   \n",
                "`page_views`       : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
                "`fpl_points`       : FPL points accumulated over the previous season (https://www.premierleague.com/news/2174909)\u003cBR\u003e\n",
                "`region`           : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
                "`nationality`      : Player's nationality \u003cBR\u003e\n",
                "`new_signing`      : Whether a new signing for 2017/18 (till 20th July)  \n",
                "`new_foreign`      : Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
                "`club_id`          : a numerical version of the Club feature  \n",
                "\n",
                "### Our goal\n",
                "To construct and fit a model that predicts the players' `market value` using all or part of the features in the given data.\n",
                "\n",
                "### Our return variable\n",
                "\n",
                "`market_value`: As on transfermrkt.com on July 20th, 2017 in Â£M (British pounds)\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inspect the Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"data/league_data.csv\")\n",
                "df_original = df.copy() # to recover later\n",
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feature Engineering"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 'Big Club'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The Football Association have a hunch that being in a **big club** affects the market value of a player, so they want us to construct and include this feature in our model. We also noticed that `club_id` had 20 categories which may be more than we might want to one-hot-encode due to the small amount of training data and the increased risk overfitting that comes with more features.\n",
                "\u003cBR\u003e\u003cBR\u003e\n",
                "Until 2017, big clubs were the following:\n",
                "\u003cBR\u003e\n",
                "    \n",
                "```\n",
                "'Arsenal', 'Chelsea', 'Liverpool', 'Manchester+City', 'Manchester+United', 'Tottenham'\n",
                "```\n",
                "\n",
                "**Create a new binary categorical variable named `big_clubs` with values $1$ if a club belongs to the Top 6 clubs and $0$ otherwise.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "big_clubs = ['Arsenal', 'Chelsea',\n",
                "             'Liverpool', 'Manchester+City',\n",
                "             'Manchester+United', 'Tottenham']\n",
                "\n",
                "df['big_club'] = df.club.isin(big_clubs).astype(int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# check\n",
                "# Check out your new `big_club` feature!\n",
                "df.head(2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Does being in a big club affect market value?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(3,2))\n",
                "ax = sns.histplot(data=df, x='market_value', hue='big_club', stat='proportion', kde=True)\n",
                "ax.set_title(\"Does being in a big club affect market value?\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Select Relevant Features and Fit a Model!\n",
                "\n",
                "Right now we really only have the means to make use of the quantitative and binary features. We'll resolve that soon enough. But let's see how well we can do with just these:\n",
                "1. Select some relevant features you think may help predict the market price (feel free to also hand-engineer some additional features like we did with `big_club`)\n",
                "2. Separate your features, `X`, from the response, `y`\n",
                "3. Perform a `test_train_split` with 20% going to test and a `random_state` of 42\n",
                "4. Fit your model and check the training MSE and $R^2$, iterate if you need to\n",
                "5. When you are satisfied, report the test MSE and $R^2$\n",
                "\n",
                "How well did you do? It is important to avoid the temptation to peek at the test score to inform your model selection!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In case we made other changes to our dataframe before fitting the above model, let's revert back to the original data plus our big_clubs feature."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df_original.copy()\n",
                "df['big_club'] = df.club.isin(big_clubs).astype(int)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### One-hot Encoding of Categorical Variables"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before we can move on to modeling, all predictors we'd like to use must be represented numerically. Some of our predictors, like `nationality`, are currently encoded as strings. Others, like `region` are currently encoded as integers but are actually categorical.\n",
                "\n",
                "We should identify all categorical columns and then *one-hot encode* them. That is, a categorical feature column with N categories should be converted to N-1 binary and mutually exclusive columns. Having a $1$ in the 1st column corresponds to an observation belonging to the 1st class, a $1$ in the 2nd column if it belongs to the 2nd class, and so on. Having all N-1 columns set to $0$ corresponds to belonging to the N-th class. In this way, N-1 binary features can be used to represent N different classes. \n",
                "\n",
                "The first step is to designate a list of `categorical_cols` which we would like to encode/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# do not forget to add 'big_club' to our features\n",
                "# remove\n",
                "categorical_cols = ['position_cat', 'new_signing', 'big_club', 'region'] "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Doing the encoding 'by hand' would be a pain. Luckily, we have a few options."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Pandas `get_dummies`\n",
                "\n",
                "The Pandas `get_dummies` function makes one-hot encoding very easy. We just need to specify which columns are to be encoded with the `columns` argument.\n",
                "\n",
                "By default, this method does *not* drop one of the resulting N columns and their values are encoded as booleans. We can tweak this behavior using the `drop_first` and `dtype` arguments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int).head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### SKLearn `OneHotEncoder`\n",
                "\n",
                "**OneHotEncoder vs. pandas get_dummies**\n",
                "\n",
                "SKLearn's OneHotEncoder and pandas' get_dummies both convert categorical variables into binary columns, but they differ in key ways:\n",
                "\n",
                "- **OneHotEncoder** works with **numpy arrays** and can be integrated into sklearn pipelines.\n",
                "- **get_dummies** operates on pandas **DataFrames** and is often used in data preprocessing outside of pipelines.\n",
                "- OneHotEncoder can handle new categories at prediction time (with 'handle_unknown' parameter), while get_dummies cannot.\n",
                "\n",
                "OneHotEncoder is particularly useful in **pipelines** because it can be applied consistently to both training and test data, ensuring the same encoding across all stages of your model.\n",
                "\n",
                "**Sklearn Transformer API**\n",
                "\n",
                "Sklearn's transformer API follows a consistent pattern:\n",
                "\n",
                "- fit(X, y=None): Learns the parameters of the transformation from the data.\n",
                "- transform(X): Applies the learned transformation to the data.\n",
                "- fit_transform(X, y=None): Combines fit and transform in one step, often more efficient.\n",
                "- get_feature_names_out(): Returns the names of the output features after transformation.\n",
                "\n",
                "This API is similar to model fitting (fit) and prediction (transform), making it intuitive and consistent across different sklearn components.\n",
                "\n",
                "Transformers can be used individually or as part of a pipeline, allowing for seamless integration of data preprocessing and model training steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
                "ohe_matrix = ohe.fit_transform(df[categorical_cols])\n",
                "type(ohe_matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "ohe_matrix.shape, df[categorical_cols].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the names of the newly created variables\n",
                "ohe.get_feature_names_out()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# make into Dataframe\n",
                "df_ohe = pd.DataFrame(ohe_matrix, columns=ohe.get_feature_names_out())\n",
                "df_ohe.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scaling Numeric Features\n",
                "\n",
                "#### Feature Scaling with StandardScaler\n",
                "\n",
                "Feature scaling is crucial in many machine learning algorithms, particularly for:\n",
                "\n",
                "- Polynomial models: Scaling prevents under- and over-flow problems when taking predictors to high degrees.\n",
                "- kNN models: Scaling ensures all features contribute equally to distance calculations, preventing features with larger scales from dominating.\n",
                "\n",
                "**Why Scaling Matters**\n",
                "- Improves convergence speed for gradient-based algorithms (less of a concern for us now)\n",
                "- Ensures features contribute proportionally to the model, based on their information content rather than their scale.\n",
                "\n",
                "**inverse_transform Method**\n",
                "The inverse_transform method allows you to revert scaled data back to its original scale, useful for interpreting results or presenting predictions in the original units.\n",
                "\n",
                "**Fitting Scalers: Train vs. Train+Test**\n",
                "There's debate on whether to fit scalers on just the training data or both training and test:\n",
                "- Fitting on train only: Prevents any information leakage from the test set (but we aren't looking at the response here anyway!)\n",
                "- Fitting on train+test: Can provide a more robust scaling, especially for small datasets (Pavlos does this).\n",
                "\n",
                "The choice often depends on the specific use case and dataset characteristics.\n",
                "\n",
                "**Other Scaling Options in sklearn**\n",
                "Besides StandardScaler, sklearn offers several other scaling methods:\n",
                "- MinMaxScaler: Scales features to a fixed range, usually [0, 1].\n",
                "- RobustScaler: Uses statistics that are robust to outliers.\n",
                "- Normalizer: Scales individual samples to have unit norm.\n",
                "- QuantileTransformer: Transforms features to follow a uniform or normal distribution.\n",
                "\n",
                "Each scaler has its strengths and is suited for different types of data and models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "numerical_cols = ['page_views', 'fpl_points', 'age'] \n",
                "\n",
                "scaler = StandardScaler().fit(df[numerical_cols])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mean and stdev are recorded after the fit\n",
                "scaler.mean_, scaler.scale_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transforming gives us a numpy array\n",
                "scaled_matrix = scaler.transform(df[numerical_cols])\n",
                "scaled_matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We can always convert back to the original scale\n",
                "scaler.inverse_transform(scaled_matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# But we've lost the column names!\n",
                "# Luckily, there is a method for getting them back\n",
                "df_scaled = pd.DataFrame(scaled_matrix, columns=scaler.get_feature_names_out())\n",
                "df_scaled.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Polynomial and Interaction Terms\n",
                "\n",
                "#### PolynomialFeatures in sklearn\n",
                "\n",
                "PolynomialFeatures is a preprocessing tool that helps us create polynomial and interaction features.\n",
                "\n",
                "**Key Parameters:**\n",
                "\n",
                "- `degree`: Controls the maximum degree of the polynomial features.\n",
                "- `include_bias`: Adds a bias (constant) term.\n",
                "   - Default is True, adding a column of 1s.\n",
                "   - **Caution:** When True, this can cause issues with linear regression models that already fit an intercept, leading to multicollinearity.\n",
                "   - Best practice: Set include_bias=False when using with sklearn's linear models.\n",
                "- `interaction_only`: If True, only interaction features are produced.\n",
                "\n",
                "\n",
                "**Note on Interactions vs. Pure Polynomials:**\n",
                "\n",
                "While you can get only interaction terms (interaction_only=True), there's no built-in way to get only polynomial terms without interactions. If needed, you'd have to create a custom transformer or manually select the desired features after transformation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Polynomial Features, 'fit' it with degree 2, and get the feature names out\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "\n",
                "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
                "poly.fit(df_scaled)\n",
                "poly.get_feature_names_out()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We can put this back into a dataframe\n",
                "df_poly = pd.DataFrame(poly.transform(df_scaled[numerical_cols]), columns=poly.get_feature_names_out())\n",
                "df_poly.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We now have our processed quantitative and categorical features\n",
                "df_scaled.shape, df_ohe.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Concatenating this back with the OHE features gives us the full design matrix\n",
                "df_design = pd.concat([df_poly, df_ohe], axis=1)\n",
                "df_design.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### SKLearn Pipelines\n",
                "\n",
                "##### Pipelines and Custom Transformers in sklearn\n",
                "\n",
                "**Pipelines**\n",
                "\n",
                "Pipelines in sklearn are a powerful tool for chaining multiple steps of data processing and modeling. They offer several advantages:\n",
                "\n",
                "- Simplification: Combine preprocessing, feature selection, and modeling in a single object.\n",
                "- Consistency: Apply the same steps to training and test data, preventing data leakage.\n",
                "- Convenience: Fit and predict on the entire workflow with a single call.\n",
                "- Grid Search: Easily perform cross-validation across all parameters in the pipeline.\n",
                "\n",
                "Example of a basic pipeline:\n",
                "```python\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('classifier', LogisticRegression())\n",
                "])\n",
                "\n",
                "pipeline.fit(X_train, y_train)\n",
                "predictions = pipeline.predict(X_test)\n",
                "```\n",
                "\n",
                "**Custom Transformers**\n",
                "\n",
                "While sklearn provides many built-in transformers, you can create custom transformers to extend functionality:\n",
                "\n",
                "- Implement specific data transformations not available in sklearn.\n",
                "- Encapsulate domain-specific feature engineering.\n",
                "- Ensure your custom preprocessing steps integrate seamlessly with sklearn pipelines.\n",
                "\n",
                "To create a custom transformer, implement a class with fit(), transform(), and fit_transform() methods:\n",
                "\n",
                "```python\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "\n",
                "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
                "    def fit(self, X, y=None):\n",
                "        # Perform any necessary computation\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        # Transform the data\n",
                "        return X_transformed\n",
                "\n",
                "# Use in a pipeline\n",
                "pipeline = Pipeline([\n",
                "    ('custom', CustomTransformer()),\n",
                "    ('model', SomeModel())\n",
                "])\n",
                "```\n",
                "\n",
                "Custom transformers allow you to incorporate any data processing logic into your machine learning workflow, making pipelines extremely flexible and powerful.\n",
                "\n",
                "Below we've created a custom transformer to give us *only* polynomial features without interaction terms."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "\n",
                "# Custom Polynomial Transformer (without interaction terms)\n",
                "class CustomPolynomialTransformer(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, degree=2):\n",
                "        self.degree = degree\n",
                "    \n",
                "    def fit(self, X, y=None):\n",
                "        self.n_features_in_ = X.shape[1]\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        X_poly = [X]\n",
                "        for d in range(2, self.degree + 1):\n",
                "            X_poly.append(X ** d)\n",
                "        return np.column_stack(X_poly)\n",
                "\n",
                "    def get_feature_names_out(self, input_features=None):\n",
                "        \"\"\"Get output feature names for transformation.\"\"\"\n",
                "        if input_features is None:\n",
                "            input_features = [f\"x{i}\" for i in range(self.n_features_in_)]\n",
                "        elif len(input_features) != self.n_features_in_:\n",
                "            raise ValueError(\"input_features should have length equal to number of features.\")\n",
                "        \n",
                "        feature_names = []\n",
                "        for feature in input_features:\n",
                "            feature_names.extend([f\"{feature}^{d}\" for d in range(1, self.degree + 1)])\n",
                "        \n",
                "        return np.array(feature_names, dtype=object)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Preprocessing Pipeline Description\n",
                "\n",
                "The pipeline below is designed to handle both our numeric and categorical features, creating polynomial terms and interactions:\n",
                "\n",
                "1. ColumnTransformer ('preprocessor'):\n",
                "   - Applies different transformations to numeric and categorical features.\n",
                "   - For numeric features: Standardizes and creates polynomial terms (up to degree 2).\n",
                "   - For categorical features: Performs one-hot encoding.\n",
                "\n",
                "2. PolynomialFeatures ('interaction'):\n",
                "   - Creates interaction terms between all features after initial preprocessing.\n",
                "   - Only generates interactions (interaction_only=True), not additional polynomial terms.\n",
                "\n",
                "The pipeline ensures consistent application of these steps to both training and test data, preventing data leakage and simplifying the workflow. It combines feature scaling, polynomial feature creation, categorical encoding, and interaction term generation in a single, reusable object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating the Preprocessing Pipeline\n",
                "numeric_features = ['age', 'page_views', 'fpl_points']\n",
                "categorical_features = ['position_cat', 'region', 'club_id', 'new_foreign', 'new_signing']\n",
                "degree = 2\n",
                "\n",
                "# Numeric features pipeline\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('poly', CustomPolynomialTransformer(degree=degree))\n",
                "])\n",
                "\n",
                "# Categorical features pipeline\n",
                "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
                "\n",
                "# Combine preprocessing steps\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, numeric_features),\n",
                "        ('cat', categorical_transformer, categorical_features)\n",
                "    ])\n",
                "\n",
                "# Add interaction terms\n",
                "interaction_transformer = PolynomialFeatures(interaction_only=True, include_bias=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Putting it all together\n",
                "pipe = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer),\n",
                "])\n",
                "pipe"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train-Validation-Test Spit with Stratification\n",
                "\n",
                "**Stratification** in data splitting ensures that the distribution of the target variable (market value) is similar across train, validation, and test sets. We bin the target values to create balanced strata, which helps maintain representative subsets for model training and evaluation. It is also possible to stratify on predictor variables.\n",
                "\n",
                "In our example below, the y_bins are split alongside X and y to maintain consistent stratification across multiple splits. In the first split, y_bins ensures the test set is representative. In the second split, y_bins_train_val is used to further stratify the training and validation sets. This approach preserves the distribution of the target variable across all subsets, ensuring each is representative of the original data distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare the features (X) and target variable (y)\n",
                "X = df.drop('market_value', axis=1)\n",
                "y = df['market_value']\n",
                "\n",
                "# Create bins for stratification\n",
                "y_bins = pd.cut(y, bins=5, labels=False)\n",
                "\n",
                "# Perform stratified split\n",
                "X_train_val, X_test, y_train_val, y_test, y_bins_train_val, y_bins_test = train_test_split(\n",
                "    X, y, y_bins, test_size=0.2, stratify=y_bins, random_state=42)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_train_val, y_train_val, test_size=0.25, stratify=y_bins_train_val, random_state=42)\n",
                "\n",
                "print(\"Training set shape:\", X_train.shape)\n",
                "print(\"Validation set shape:\", X_val.shape)\n",
                "print(\"Test set shape:\", X_test.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Later, once we've selected the best model using the validation set, we will *refit it on the combined training and validation data*. This approach maximizes the use of available data for the final model, potentially improving its performance and generalization ability. The test set remains untouched until final evaluation, providing an unbiased assessment of the model's performance on unseen data. \n",
                "\n",
                "For this reason, it is nice to keep variables from the first split that contains both the train and validation data (we call them `X_train_val` and `y_train_val`. Of course, you could always simply concatenate the train and validation sets back together."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "OK, let's try fitting on our design matrix from the pipeline!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipe.fit(X)\n",
                "X_train_design = pipe.transform(X_train)\n",
                "X_val_design = pipe.transform(X_val)\n",
                "X_test_design = pipe.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr = LinearRegression().fit(X_train_design, y_train)\n",
                "lr.score(X_train_design, y_train), lr.score(X_val_design, y_val)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr.score(X_test_design, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Oops! we've clearly overfit. Perhaps a greedy, **forward-step feature selection** approach might improve things."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "def forward_feature_selection(X, y, X_val, y_val, model, max_features=10):\n",
                "    features = []\n",
                "    best_score = float('-inf')\n",
                "    \n",
                "    while len(features) \u003c max_features:\n",
                "        best_new_feature = None\n",
                "        for feature in range(X.shape[1]):\n",
                "            if feature not in features:\n",
                "                current_features = features + [feature]\n",
                "                model.fit(X[:, current_features], y)\n",
                "                score = model.score(X_val[:, current_features], y_val)\n",
                "                \n",
                "                if score \u003e best_score:\n",
                "                    best_score = score\n",
                "                    best_new_feature = feature\n",
                "        \n",
                "        if best_new_feature is None:\n",
                "            break\n",
                "        \n",
                "        features.append(best_new_feature)\n",
                "        print(f\"Added feature: {best_new_feature}, Score: {best_score:.4f}\")\n",
                "    \n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model selection and evaluation\n",
                "model = LinearRegression()\n",
                "max_features = 5\n",
                "\n",
                "print(f\"\\nPerforming forward feature selection for Linear Regression:\")\n",
                "pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer)\n",
                "])\n",
                "X_train_processed = pipeline.fit_transform(X_train)\n",
                "X_val_processed = pipeline.transform(X_val)\n",
                "\n",
                "best_features = forward_feature_selection(X_train_processed, y_train, \n",
                "                                              X_val_processed, y_val, model,\n",
                "                                              max_features=max_features)\n",
                "\n",
                "final_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer),\n",
                "    ('selector', ColumnTransformer([('selector', 'passthrough', best_features)])),\n",
                "    ('model', model)\n",
                "])\n",
                "\n",
                "final_pipeline.fit(X_train, y_train)\n",
                "best_score = final_pipeline.score(X_val, y_val)\n",
                "\n",
                "best_model = final_pipeline\n",
                "\n",
                "print(f\"\\nBest model: {type(best_model.named_steps['model']).__name__}\")\n",
                "print(f\"Best validation R2 score: {best_score:.4f}\")\n",
                "print(\"Selected features:\", best_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline.get_feature_names_out()[best_features]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Model Evaluation\n",
                "best_model.fit(X_train_val, y_train_val)\n",
                "y_test_pred = best_model.predict(X_test)\n",
                "test_r2 = r2_score(y_test, y_test_pred)\n",
                "test_mse = mean_squared_error(y_test, y_test_pred)\n",
                "\n",
                "print(\"\\nFinal Model Performance on Test Set:\")\n",
                "print(f\"R2 Score: {test_r2:.4f}\")\n",
                "print(f\"Mean Squared Error: {test_mse:.4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### kNN Pipeline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pipeline Parameter Passing:\n",
                "We can set parameters for any step in the pipeline using the syntax `stepname__parameter`. In our case, we use `regressor__n_neighbors=k` to set the number of neighbors for the KNeighborsRegressor. This is a key feature that allows us to modify pipeline parameters dynamically. We can easily change the value of k without creating a new pipeline each time.\n",
                "\n",
                "We've also created an `evaluate_knn` function that encapsulates the process of setting the k value, fitting the model, and evaluating it on both training and validation sets.\n",
                "\n",
                "This method can easily be extended to tune multiple hyperparameters simultaneously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create kNN pipeline\n",
                "knn_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('regressor', KNeighborsRegressor())\n",
                "])\n",
                "\n",
                "# Function to fit and evaluate the pipeline for a given k\n",
                "def evaluate_knn(k, X_train, y_train, X_val, y_val):\n",
                "    knn_pipeline.set_params(regressor__n_neighbors=k)\n",
                "    knn_pipeline.fit(X_train, y_train)\n",
                "    train_mse = mean_squared_error(y_train, knn_pipeline.predict(X_train))\n",
                "    val_mse = mean_squared_error(y_val, knn_pipeline.predict(X_val))\n",
                "    return train_mse, val_mse\n",
                "\n",
                "# Fit and evaluate for different k values\n",
                "k_values = range(1, 31)\n",
                "train_mses = []\n",
                "val_mses = []\n",
                "\n",
                "for k in k_values:\n",
                "    train_mse, val_mse = evaluate_knn(k, X_train, y_train, X_val, y_val)\n",
                "    train_mses.append(train_mse)\n",
                "    val_mses.append(val_mse)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(k_values, train_mses, label='Training MSE')\n",
                "plt.plot(k_values, val_mses, label='Validation MSE')\n",
                "plt.xlabel('Number of Neighbors (k)')\n",
                "plt.ylabel('Mean Squared Error')\n",
                "plt.title('kNN: Performance vs. Number of Neighbors')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# Find best k\n",
                "best_k = k_values[np.argmin(val_mses)]\n",
                "print(f\"Best k: {best_k}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Refit the best model on combined training and validation data\n",
                "best_knn_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('regressor', KNeighborsRegressor(n_neighbors=best_k))\n",
                "])\n",
                "best_knn_pipeline.fit(X_train_val, y_train_val)\n",
                "\n",
                "# Evaluate on test set\n",
                "y_pred = best_knn_pipeline.predict(X_test)\n",
                "test_mse = mean_squared_error(y_test, y_pred)\n",
                "test_r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(\"\\nFinal Model Performance on Test Set:\")\n",
                "print(f\"Mean Squared Error: {test_mse:.4f}\")\n",
                "print(f\"RÂ² Score: {test_r2:.4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A sneak peak for cross-validation and regularization for next time..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "import numpy as np\n",
                "\n",
                "def forward_feature_selectionCV(X, y, model, max_features=10, cv=5):\n",
                "    features = []\n",
                "    best_score = float('inf')  # We're minimizing MSE\n",
                "    n_features = X.shape[1]\n",
                "    \n",
                "    while len(features) \u003c max_features:\n",
                "        best_new_feature = None\n",
                "        for i in range(n_features):\n",
                "            if i not in features:\n",
                "                current_features = features + [i]\n",
                "                if isinstance(X, pd.DataFrame):\n",
                "                    X_subset = X.iloc[:, current_features]\n",
                "                else:\n",
                "                    X_subset = X[:, current_features]\n",
                "                \n",
                "                scores = cross_val_score(model, X_subset, y, cv=cv, scoring='neg_mean_squared_error')\n",
                "                score = -scores.mean()  # Convert to positive MSE\n",
                "                \n",
                "                if score \u003c best_score:  # We're minimizing MSE\n",
                "                    best_score = score\n",
                "                    best_new_feature = i\n",
                "        \n",
                "        if best_new_feature is None:\n",
                "            break\n",
                "        \n",
                "        features.append(best_new_feature)\n",
                "        print(f\"Added feature: {best_new_feature}, MSE: {best_score:.4f}\")\n",
                "    \n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model selection and evaluation\n",
                "model = LinearRegression()\n",
                "max_features = 3\n",
                "cv = 7\n",
                "\n",
                "print(f\"\\nPerforming forward feature selection for Linear Regression:\")\n",
                "pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer)\n",
                "])\n",
                "\n",
                "pipeline.fit(X)\n",
                "X_train_val_processed = pipeline.transform(X_train_val)\n",
                "\n",
                "best_features = forward_feature_selectionCV(X_train_val_processed,\n",
                "                                                y_train_val,\n",
                "                                                model, max_features=max_features,\n",
                "                                                cv=5)\n",
                "\n",
                "final_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer),\n",
                "    ('selector', ColumnTransformer([('selector', 'passthrough', best_features)])),\n",
                "    ('model', model)\n",
                "])\n",
                "\n",
                "final_pipeline.fit(X_train, y_train)\n",
                "best_score = final_pipeline.score(X_val, y_val)\n",
                "best_model = final_pipeline\n",
                "\n",
                "print(f\"Best validation R2 score: {best_score:.4f}\")\n",
                "print(\"Selected features:\", pipeline.get_feature_names_out()[best_features])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final CV Model Evaluation\n",
                "best_model.fit(X_train_val, y_train_val)\n",
                "y_test_pred = best_model.predict(X_test)\n",
                "test_r2 = r2_score(y_test, y_test_pred)\n",
                "test_mse = mean_squared_error(y_test, y_test_pred)\n",
                "\n",
                "print(\"\\nFinal Model Performance on Test Set:\")\n",
                "print(f\"R2 Score: {test_r2:.4f}\")\n",
                "print(f\"Mean Squared Error: {test_mse:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LassoCV\n",
                "# Create the full pipeline including LassoCV\n",
                "lasso_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('interaction', interaction_transformer),\n",
                "    ('lasso', LassoCV(cv=5, max_iter=5000, random_state=42))\n",
                "])\n",
                "\n",
                "# Fit the pipeline on the training data\n",
                "lasso_pipeline.fit(X_train_val, y_train_val)\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred = lasso_pipeline.predict(X_test)\n",
                "\n",
                "# Calculate performance metrics\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(\"LassoCV Model Performance on Test Set:\")\n",
                "print(f\"Mean Squared Error: {mse:.4f}\")\n",
                "print(f\"R-squared Score: {r2:.4f}\")\n",
                "\n",
                "print(f\"Selected alpha: {lasso_pipeline.named_steps['lasso'].alpha_:.6f}\")\n",
                "\n",
                "# Print non-zero coefficients\n",
                "feature_names = lasso_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
                "feature_names = lasso_pipeline.named_steps['interaction'].get_feature_names_out(feature_names)\n",
                "lasso_coef = lasso_pipeline.named_steps['lasso'].coef_\n",
                "\n",
                "print(\"\\nNon-zero coefficients:\")\n",
                "for name, coef in zip(feature_names, lasso_coef):\n",
                "    if coef != 0:\n",
                "        print(f\"{name}: {coef:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
